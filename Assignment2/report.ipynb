{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DL Assignment #2\n",
    "\n",
    "> ‚ú®\n",
    "> \n",
    "> `ü•ùName` \n",
    "> - Chenrui Fan\n",
    "> \n",
    "> `üçâStudent ID` \n",
    "> - 23-125-818\n",
    "> \n",
    "> `üçëGithub`\n",
    "> - https://github.com/Sosekie/DL_Assignment\n",
    "> \n",
    "> ‚ú®"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1 ‚Äì Baseline\n",
    "\n",
    "The baseline consists of a simple convolutional image encoder followed by 2 RNN layers stacked\n",
    "vertically one on top of another. The RNN decodes the image code into a sequence of word\n",
    "tokens by treating the image code as the first input token (similar to a <SOS> token). The\n",
    "hidden state of the RNN is initialized with zeros. For more details refer to the code.\n",
    "\n",
    "### To Do \n",
    "\n",
    "The first task is to get familiar with the code base by training the baseline and\n",
    "calculating its BLEU scores (evaluating it on the validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Training the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreal_chenrui_fan\u001b[0m (\u001b[33mgood-pig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./logs/model_1/wandb/run-20240510_195620-n8ugv7fh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_1\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/n8ugv7fh\u001b[0m\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 22.2%             \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss ‚ñà‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss ‚ñà‚ñÜ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss 2.92944\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss 3.53386\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmodel_1\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/n8ugv7fh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 200 media file(s), 689 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./logs/model_1/wandb/run-20240510_195620-n8ugv7fh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python train.py --device-id=0 --config-file-path=./configs/config_model_1.yaml \\\n",
    "--experiment-name=model_1 --num-epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> calculating BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python evaluate.py -d 0 --checkpoint-path=./checkpoints/model_1/model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "\n",
    "Although the baseline provides a simple and logical encoder/decoder architecture, it is often\n",
    "difficult to train such a system end-to-end. At the beginning of the training the image encoder\n",
    "does not extract useful information from the input image and the RNN has to hallucinate the\n",
    "output almost from nothing. Moreover, the training signal from the loss has to pass through\n",
    "all recurrent decoding iterations to reach the encoder. This complicates the training of the\n",
    "encoder and hence the whole model.\n",
    "\n",
    "To overcome this issue, following the good practices of representation and transfer learning, we\n",
    "propose to substitute the image encoder with a pre-trained network. Although this network\n",
    "can be further fine-tuned, this is not necessary, and it can be frozen for simplicity. As the\n",
    "image backbone, we suggest using DINOv2, as it is proven to have learned a great image\n",
    "representation\n",
    "\n",
    "### To Do \n",
    "\n",
    "Substitute the image backbone with DINOv2. Use the final <CLS> token from the\n",
    "DINOv2‚Äôs image representation as your image encoding. Freeze the backbone and\n",
    "train the decoder from scratch.\n",
    "\n",
    "Notice that thanks to the modular implementation, to solve this task, it is sufficient\n",
    "to change only the encoder‚Äôs code.\n",
    "\n",
    "Explore the official DINOv2‚Äôs GitHub to see how to load and use the pre-trained\n",
    "models. Tutorial 10 may also help you with this.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Using DINOv2 as encoder and train the decoder from scratch, freeze() is called in train.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreal_chenrui_fan\u001b[0m (\u001b[33mgood-pig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./logs/model_2/wandb/run-20240511_002337-21c8uvou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/21c8uvou\u001b[0m\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 9.6%             \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss ‚ñà‚ñÖ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss ‚ñà‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss 3.02027\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss 3.12115\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmodel_2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/21c8uvou\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 200 media file(s), 572 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./logs/model_2/wandb/run-20240511_002337-21c8uvou/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python train.py --device-id=0 --config-file-path=./configs/config_model_2.yaml \\\n",
    "--experiment-name=model_2 --num-epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> calculating BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python evaluate.py -d 0 --checkpoint-path=./checkpoints/model_2/model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3\n",
    "\n",
    "We know that DINOv2 provides a great signal. So, in order to improve the model, we need to\n",
    "modify the decoder. A simple RNN seems not a great choice. What about one of the gated\n",
    "recurrent counterparts discussed in the lectures and the tutorials?\n",
    "\n",
    "### To Do \n",
    "\n",
    "Given the encoder from Model 2, replace the RNN in the decoder with a gated\n",
    "recurrency (LSTM, GRU, etc.). Train the model and try different number of layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Decoder Model: LSTM\\\n",
    "> Num of Layers: 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreal_chenrui_fan\u001b[0m (\u001b[33mgood-pig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./logs/model_3/wandb/run-20240511_012338-ih0bansi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_3\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/ih0bansi\u001b[0m\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 15.7%             \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss ‚ñà‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss ‚ñà‚ñá‚ñÜ‚ñÜ‚ñÜ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss 2.90281\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss 3.03453\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmodel_3\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/ih0bansi\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 200 media file(s), 622 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./logs/model_3/wandb/run-20240511_012338-ih0bansi/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python train.py --device-id=0 --config-file-path=./configs/config_model_3.yaml \\\n",
    "--experiment-name=model_3 --num-epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> calculating BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python evaluate.py -d 0 --checkpoint-path=./checkpoints/model_3/model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4\n",
    "\n",
    "So far the decoder was conditioned on the output of the encoder by feeding it as the first input\n",
    "token to the RNN. This complicates the task for the RNN, as it has to distinguish the cases\n",
    "when the input is a word token and when the input is an image encoding. Are there other ways\n",
    "to condition the decoder on the encoder‚Äôs output?\n",
    "\n",
    "### To Do \n",
    "\n",
    "Propose a different scheme for conditioning the decoder on the image encoding and\n",
    "train the model.\n",
    "\n",
    "You will be asked to justify your idea in the report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ÂàùÂßãÂåñÈöêËóèÁä∂ÊÄÅÔºö‰∏ÄÁßçÂ∏∏ËßÅÁöÑÊñπÊ≥ïÊòØ‰ΩøÁî®ÂõæÂÉèÁºñÁ†ÅÊù•ÂàùÂßãÂåñËß£Á†ÅÂô®ÁöÑÈöêËóèÁä∂ÊÄÅ„ÄÇÂú®ËøôÁßçÊÉÖÂÜµ‰∏ãÔºåÂõæÂÉèÁºñÁ†ÅÁõ¥Êé•‰Ωú‰∏∫LSTMÊàñGRUÁöÑÂàùÂßãÁä∂ÊÄÅÔºåËøôÊ†∑Ëß£Á†ÅÂô®Âú®ÁîüÊàêÊØè‰∏™ÂçïËØçÊó∂ÈÉΩÈöêÂºèÂú∞ÂåÖÂê´‰∫ÜÂõæÂÉèÁöÑ‰ø°ÊÅØ„ÄÇËøôÁßçÊñπÊ≥ïÁÆÄÂåñ‰∫ÜÊ®°ÂûãÔºåÂõ†‰∏∫Ëß£Á†ÅÂô®‰∏çÈúÄË¶ÅÂå∫ÂàÜËæìÂÖ•ÊòØÂõæÂÉèÁºñÁ†ÅËøòÊòØÊñáÊú¨Ê†áËÆ∞„ÄÇ\n",
    "\n",
    "‰ΩøÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂ÔºöÂè¶‰∏ÄÁßçÊñπÊ≥ïÊòØÈááÁî®Ê≥®ÊÑèÂäõÊú∫Âà∂„ÄÇÂú®ËøôÁßçÊñπÊ°à‰∏≠ÔºåËß£Á†ÅÂô®Âú®ÁîüÊàêÊØè‰∏™ËØçÊó∂ÈÉΩ‰ºöÂØπÂõæÂÉèÁºñÁ†ÅËøõË°å‚ÄúÊ≥®ÊÑè‚ÄùÔºå‰ªéËÄåÈÄâÊã©ÊÄßÂú∞ÂÖ≥Ê≥®ÂõæÂÉèÁöÑ‰∏çÂêåÈÉ®ÂàÜ„ÄÇËøôÁßçÂä®ÊÄÅÁöÑ‰ø°ÊÅØÊï¥ÂêàÂèØ‰ª•Â∏ÆÂä©Ê®°ÂûãÊõ¥Â•ΩÂú∞ÁêÜËß£ÂõæÂÉè‰∏éÁîüÊàêÁöÑÊñáÊú¨‰πãÈó¥ÁöÑÂÖ≥Á≥ª„ÄÇ\n",
    "\n",
    "ÁºñÁ†ÅÂô®-Ëß£Á†ÅÂô®‰∫§‰∫íÔºö‰Ω†ÂèØ‰ª•Âú®Ëß£Á†ÅÂô®ÁöÑÊØè‰∏™Êó∂Èó¥Ê≠•‰ΩøÁî®ÂõæÂÉèÁºñÁ†Å‰∏éÂΩìÂâçËæìÂÖ•ÁöÑÂµåÂÖ•ÂêëÈáèËøõË°åËûçÂêàÔºå‰æãÂ¶ÇÈÄöËøáÂä†ÊùÉÂíåÊàñ‰∏≤ËÅîÔºàconcatenationÔºâÂêéÂÜçËæìÂÖ•Âà∞LSTM‰∏≠„ÄÇËøôÁßçÊñπÊ≥ïÂèØ‰ª•Âú®Ëß£Á†ÅËøáÁ®ã‰∏≠‰∏çÊñ≠Âú∞Â∞ÜÂõæÂÉè‰ø°ÊÅØËûçÂÖ•Âà∞ÊñáÊú¨ÁîüÊàêËøáÁ®ã‰∏≠„ÄÇ\n",
    "\n",
    "Êù°‰ª∂ÊâπÈáèÂΩí‰∏ÄÂåñÔºö‰ΩøÁî®Êù°‰ª∂ÊâπÈáèÂΩí‰∏ÄÂåñÔºàConditional Batch NormalizationÔºâÊàñËÄÖÊù°‰ª∂Â±ÇÂΩí‰∏ÄÂåñÔºàConditional Layer NormalizationÔºâÔºåÈÄöËøáÂõæÂÉèÁºñÁ†ÅÊù•Ë∞ÉÊï¥ÂΩí‰∏ÄÂåñÂ±ÇÁöÑÂèÇÊï∞„ÄÇËøôÁßçÊñπÂºèÂèØ‰ª•Âú®ÊØè‰∏™Â±ÇÊ¨°‰∏äË∞ÉÊï¥Á•ûÁªèÁΩëÁªúÁöÑË°å‰∏∫Ôºå‰ΩøÂÖ∂Êõ¥Â•ΩÂú∞ÈÄÇÂ∫î‰ªéÂõæÂÉèÂà∞ÊñáÊú¨ÁöÑÊò†Â∞Ñ„ÄÇ\n",
    "\n",
    "Â§öÊ®°ÊÄÅËûçÂêàÂ±ÇÔºöËÆæÁΩÆ‰∏ìÈó®ÁöÑÂ§öÊ®°ÊÄÅËûçÂêàÂ±ÇÔºåÂ∞ÜÂõæÂÉèÁºñÁ†Å‰∏éÊñáÊú¨ÂµåÂÖ•ËøõË°åÊúâÊïàÁªìÂêà„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•‰ΩøÁî®Â§öÂ§¥Ê≥®ÊÑèÂäõÊú∫Âà∂Êù•ÂÆûÁé∞Ëøô‰∏ÄÁÇπÔºåÂÖÅËÆ∏Ê®°ÂûãÂú®ÊñáÊú¨ÁîüÊàêÊó∂Êé¢Á¥¢ÂíåÂà©Áî®ÂõæÂÉè‰∏éÊñáÊú¨‰πãÈó¥ÁöÑÂ§çÊùÇÂÖ≥Á≥ª„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mreal_chenrui_fan\u001b[0m (\u001b[33mgood-pig\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.0 is available!  To upgrade, please run:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.16.6\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m./logs/model_4/wandb/run-20240511_230605-eu04kqlh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mmodel_4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run at \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/eu04kqlh\u001b[0m\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: W&B sync reduced upload amount by 16.6%             \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss ‚ñà‚ñÖ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss ‚ñà‚ñá‚ñá‚ñÜ‚ñÜ‚ñÖ‚ñÖ‚ñÑ‚ñÑ‚ñÑ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÉ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÇ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: train/cross_entropy_loss 2.83524\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   val/cross_entropy_loss 3.00376\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: üöÄ View run \u001b[33mmodel_4\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2/runs/eu04kqlh\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ‚≠êÔ∏è View project at: \u001b[34m\u001b[4mhttps://wandb.ai/good-pig/Assignment2\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 6 W&B file(s), 200 media file(s), 632 artifact file(s) and 0 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./logs/model_4/wandb/run-20240511_230605-eu04kqlh/logs\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "! python train.py --device-id=0 --config-file-path=./configs/config_model_4.yaml \\\n",
    "--experiment-name=model_4 --num-epochs=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> calculating BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "! python evaluate.py -d 0 --checkpoint-path=./checkpoints/model_4/model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 5\n",
    "\n",
    "Who are we trying to fool by training an RNN for language modeling in 2024?\n",
    "\n",
    "### To Do \n",
    "\n",
    "Replace the RNN with a Transformer and train the model. Be careful with how you\n",
    "condition the transformer on the image encoding.\n",
    "\n",
    "You may use the TransformerEncoderLayer class to build the decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 32, 512])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "\n",
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n",
    "src = torch.rand(10, 32, 512)\n",
    "out = encoder_layer(src)\n",
    "print(out.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "Epoch 01 | train:   0%|                              | 0/139 [00:00<?, ?batch/s]embeddings.size(): torch.Size([256, 28, 512])\n",
      "output.size(): torch.Size([256, 28, 512])\n",
      "Epoch 01 | train:   1%| | 1/139 [00:02<06:50,  2.97s/batch, cross_entropy_loss: embeddings.size(): torch.Size([256, 32, 512])\n",
      "output.size(): torch.Size([256, 32, 512])\n",
      "Epoch 01 | train:   1%| | 2/139 [00:03<03:03,  1.34s/batch, cross_entropy_loss: embeddings.size(): torch.Size([256, 27, 512])\n",
      "output.size(): torch.Size([256, 27, 512])\n",
      "Epoch 01 | train:   2%| | 3/139 [00:03<01:51,  1.22batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 27, 512])\n",
      "output.size(): torch.Size([256, 27, 512])\n",
      "Epoch 01 | train:   3%| | 4/139 [00:03<01:18,  1.72batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 27, 512])\n",
      "output.size(): torch.Size([256, 27, 512])\n",
      "Epoch 01 | train:   4%| | 5/139 [00:03<00:59,  2.24batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 29, 512])\n",
      "output.size(): torch.Size([256, 29, 512])\n",
      "Epoch 01 | train:   4%| | 6/139 [00:04<00:51,  2.60batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 29, 512])\n",
      "output.size(): torch.Size([256, 29, 512])\n",
      "Epoch 01 | train:   5%| | 7/139 [00:04<00:47,  2.77batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 24, 512])\n",
      "output.size(): torch.Size([256, 24, 512])\n",
      "Epoch 01 | train:   6%| | 8/139 [00:04<00:40,  3.23batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 34, 512])\n",
      "output.size(): torch.Size([256, 34, 512])\n",
      "Epoch 01 | train:   6%| | 9/139 [00:04<00:35,  3.62batch/s, cross_entropy_loss: embeddings.size(): torch.Size([256, 29, 512])\n",
      "output.size(): torch.Size([256, 29, 512])\n",
      "Epoch 01 | train:   7%| | 10/139 [00:04<00:32,  3.92batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 24, 512])\n",
      "output.size(): torch.Size([256, 24, 512])\n",
      "Epoch 01 | train:   8%| | 11/139 [00:05<00:30,  4.17batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 24, 512])\n",
      "output.size(): torch.Size([256, 24, 512])\n",
      "Epoch 01 | train:   9%| | 12/139 [00:05<00:29,  4.36batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 27, 512])\n",
      "output.size(): torch.Size([256, 27, 512])\n",
      "Epoch 01 | train:   9%| | 13/139 [00:05<00:27,  4.53batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 25, 512])\n",
      "output.size(): torch.Size([256, 25, 512])\n",
      "Epoch 01 | train:  10%| | 14/139 [00:05<00:26,  4.66batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 24, 512])\n",
      "output.size(): torch.Size([256, 24, 512])\n",
      "Epoch 01 | train:  11%| | 15/139 [00:05<00:26,  4.74batch/s, cross_entropy_loss:embeddings.size(): torch.Size([256, 28, 512])\n",
      "output.size(): torch.Size([256, 28, 512])\n",
      "Epoch 01 | train:  11%| | 15/139 [00:06<00:26,  4.74batch/s, cross_entropy_loss:^C\n",
      "Traceback (most recent call last):                                              \n",
      "  File \"/mnt/f/GitHub/DL_Assignment/Assignment2/train.py\", line 81, in <module>\n",
      "    main(args=args, config=config)\n",
      "  File \"/mnt/f/GitHub/DL_Assignment/Assignment2/train.py\", line 69, in main\n",
      "    trainer.train(train_dataloader=train_dataloader,\n",
      "  File \"/mnt/f/GitHub/DL_Assignment/Assignment2/training/trainer.py\", line 25, in train\n",
      "    self.run_epoch(dataloader=train_dataloader, epoch=epoch, phase='train')\n",
      "  File \"/mnt/f/GitHub/DL_Assignment/Assignment2/training/trainer.py\", line 49, in run_epoch\n",
      "    self._optimize(objective)\n",
      "  File \"/mnt/f/GitHub/DL_Assignment/Assignment2/training/trainer.py\", line 83, in _optimize\n",
      "    objective.backward()\n",
      "  File \"/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/torch/_tensor.py\", line 522, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/torch/autograd/__init__.py\", line 266, in backward\n",
      "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "! python train.py --device-id=0 --config-file-path=./configs/config_model_5.yaml \\\n",
    "--experiment-name=model_5 --num-epochs=1 --no-log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> calculating BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/fcr/.cache/torch/hub/facebookresearch_dinov2_main    \n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/swiglu_ffn.py:51: UserWarning: xFormers is not available (SwiGLU)\n",
      "  warnings.warn(\"xFormers is not available (SwiGLU)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/attention.py:33: UserWarning: xFormers is not available (Attention)\n",
      "  warnings.warn(\"xFormers is not available (Attention)\")\n",
      "/home/fcr/.cache/torch/hub/facebookresearch_dinov2_main/dinov2/layers/block.py:40: UserWarning: xFormers is not available (Block)\n",
      "  warnings.warn(\"xFormers is not available (Block)\")\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 2-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 3-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n",
      "/home/fcr/anaconda3/envs/dl_a2/lib/python3.11/site-packages/nltk/translate/bleu_score.py:552: UserWarning: \n",
      "The hypothesis contains 0 counts of 4-gram overlaps.\n",
      "Therefore the BLEU score evaluates to 0, independently of\n",
      "how many N-gram overlaps of lower order it contains.\n",
      "Consider using lower n-gram order or use SmoothingFunction()\n",
      "  warnings.warn(_msg)\n"
     ]
    }
   ],
   "source": [
    "! python evaluate.py -d 0 --checkpoint-path=./checkpoints/model_5/model.pth.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 6\n",
    "\n",
    "Along with the <CLS> token, DINOv2 also produces spatial tokens that capture more fine-\n",
    "grained and local information in the image. These local features may be a better fit for our\n",
    "model in contrast to the global <CLS> token. However, now instead of one encoding per image,\n",
    "we have a sequence of image codes, from which we need to decode the caption. Does this\n",
    "remind you of anything from the lectures/tutorials? What if we could attend to those tokens\n",
    "during the decoding procedure?\n",
    "\n",
    "### To Do\n",
    "\n",
    "Replace the <CLS> token of DINOv2 with the spatial features. Condition on those\n",
    "tokens with the Cross-Attention procedure. Train the model.\n",
    "\n",
    "You may use the TransformerDecoderLayer or the MultiheadAttention classes to build\n",
    "the decoder network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_a2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
